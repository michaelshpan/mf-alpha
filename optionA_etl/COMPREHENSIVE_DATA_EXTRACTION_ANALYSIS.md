# Comprehensive Data Extraction Analysis: pilot_pipeline_tradefeeds.py

## Overview
This document provides a detailed breakdown of data extraction, processing, and computation for all 37 fields in the output parquet file generated by `pilot_pipeline_tradefeeds.py`.

---

## Field-by-Field Analysis

### 1. **date**
- **Source**: Tradefeeds OHLCV API
- **Collection Method**: 
  - **Endpoint**: `https://data.tradefeeds.com/api/v1/historical_ohlcv`
  - **Headers**: `{'accept': 'application/json'}`
  - **Parameters**: `{'key': api_key, 'ticker': ticker, 'date_from': 'YYYY-MM-DD', 'date_to': 'YYYY-MM-DD'}`
  - **Rate Limiting**: 2-second intervals between requests
- **Processing**: Extracted from daily OHLCV JSON response field "date", converted to datetime and set as month start date
- **Deduplication**: Yes - duplicates removed by class_id + month_end combination using factor completeness ranking
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - used as-is from API response

### 2. **month_end**
- **Source**: Tradefeeds OHLCV API  
- **Collection Method**: Same as above
- **Processing**: Calculated from daily OHLCV data by grouping by calendar month (`df["period"] = df["date"].dt.to_period("M")`) and finding last trading day of each month
- **Deduplication**: Yes - same as above
- **Data Overrides**: No overrides apply
- **Post Calculations**: Derived using `pd.offsets.MonthEnd(0)` to get month-end date from trading dates

### 3. **series_id**
- **Source**: SEC Investment Company Series/Class CSV files + pilot configuration
- **Collection Method**: 
  - **Primary Sources**: Downloads from `https://www.sec.gov/files/investment/data/other/investment-company-series-class-information/investment-company-series-class-{YEAR}.csv`
  - **Headers**: `{"User-Agent": SEC_USER_AGENT, "Accept-Encoding": "gzip, deflate", "Host": "www.sec.gov", "Connection": "keep-alive"}`
  - **Discovery Method**: Falls back to scraping SEC pages if direct downloads fail
  - **Caching**: Uses local cache file `data/series_class_mapping_cache.csv`
- **Processing**: 
  - Downloads multiple years (2025, 2024, 2023, 2022, 2021) and combines
  - Column normalization: lowercase, replace spaces/hyphens with underscores
  - Maps class_id to series_id using SeriesClassMapper
  - Cross-referenced with pilot configuration YAML
- **Deduplication**: Yes - inherits deduplication from main data
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - direct mapping lookup

### 4. **series_name**
- **Source**: Tradefeeds OHLCV API
- **Collection Method**: Same endpoint as above
- **Processing**: Extracted as metadata from API response, default format "Fund for {ticker}"
- **Deduplication**: Yes - inherits deduplication
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - used as-is

### 5. **ticker**
- **Source**: SEC series/class mapping + ticker lookup
- **Collection Method**: Same SEC CSV downloads as series_id
- **Processing**: 
  - Ticker symbols extracted from SEC CSV files
  - Mapped to class_ids through SeriesClassMapper._build_ticker_lookup()
  - Creates structure: series_id -> [{"class_id": "...", "ticker": "..."}]
- **Deduplication**: Yes - inherits deduplication
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - direct lookup from mapping

### 6. **return**
- **Source**: Tradefeeds OHLCV API
- **Collection Method**: Same endpoint as above
- **Processing**: 
  - Monthly returns calculated from daily "adjustclose" prices
  - Groups by period: `df.groupby("period").agg(adjustclose=("adjustclose", "last"))`
  - Calculates returns: `monthly["return"] = monthly["adjustclose"].pct_change()`
- **Deduplication**: Yes - duplicates removed by class_id + month_end
- **Data Overrides**: No overrides apply
- **Post Calculations**: `monthly_return = adjustclose.pct_change()` - percentage change in dividend-adjusted closing prices

### 7. **report_date**
- **Source**: Tradefeeds OHLCV API
- **Collection Method**: Same endpoint as above
- **Processing**: Set equal to month_end date from OHLCV data
- **Deduplication**: Yes - inherits deduplication
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - copied from month_end

### 8. **adjustclose**
- **Source**: Tradefeeds OHLCV API
- **Collection Method**: Same endpoint as above
- **Processing**: 
  - Extracted from daily OHLCV JSON response field "adjustclose"
  - Converted to numeric: `pd.to_numeric(df[col], errors="coerce").fillna(0.0)`
  - Aggregated to monthly using `.last()` method for each month
- **Deduplication**: Yes - inherits deduplication
- **Data Overrides**: No overrides apply  
- **Post Calculations**: Aggregated using `.last()` method for each month

### 9. **filing_date**
- **Source**: SEC N-PORT filings (when TNA data available)
- **Collection Method**: 
  - **Endpoint**: SEC EDGAR API via `list_recent_nport_p_accessions()`
  - **Headers**: SEC headers with User-Agent
  - **Processing**: Downloads XML filings and extracts metadata
- **Processing**: Filing date extracted from SEC EDGAR response metadata
- **Deduplication**: Yes - when multiple N-PORT filings exist for same class_id+month_end, keeps most recent filing (`keep='last'`)
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - used as-is from EDGAR metadata

### 10. **registrant_name**
- **Source**: Pilot configuration YAML file
- **Collection Method**: File read from `config/funds_pilot.yaml`
- **Processing**: Extracted from "name" field in registrants configuration using `yaml.safe_load()`
- **Deduplication**: Yes - inherits deduplication
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - used as-is from config

### 11. **total_investments**
- **Source**: SEC N-PORT-P filings
- **Collection Method**: 
  - **Method**: SEC EDGAR API download + XML parsing
  - **Function**: `parse_nport_primary_xml(xml)` from nport_parser_fixed.py
- **Processing**: 
  - Downloads N-PORT XML files
  - Parses portfolio holdings data
  - Aggregates from individual holding positions
- **Deduplication**: Yes - keeps most recent filing per class_id+month_end combination (`tna_data.sort_values(['class_id', 'month_end', 'filing_date']).drop_duplicates(subset=['class_id', 'month_end'], keep='last')`)
- **Data Overrides**: No overrides apply
- **Post Calculations**: Aggregated from individual holding positions in N-PORT filing

### 12. **sales**
- **Source**: SEC N-PORT-P filings (primary) + Tradefeeds API (fallback)
- **Collection Method**: 
  - **SEC**: Same as total_investments above
  - **Tradefeeds**: 
    - **Endpoint**: `https://data.tradefeeds.com/api/v1/mutual_fund_periodical_returns`
    - **Parameters**: `{'key': api_key, 'seriesId': series_id, 'date_from': date, 'date_to': date}`
- **Processing**: 
  - **SEC**: extracted from N-PORT XML flow data
  - **Tradefeeds**: parsed from JSON response `mon1/2/3Flow.sales` fields
  - Hybrid approach combines both sources
- **Deduplication**: Yes - SEC data preferred over Tradefeeds (`source_priority`: SEC=0, Tradefeeds=1), then deduplicated by class_id+month_end keeping first (SEC)
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - used as-is from either source

### 13. **redemptions**
- **Source**: SEC N-PORT-P filings (primary) + Tradefeeds API (fallback)
- **Collection Method**: Same hybrid approach as sales
- **Processing**: 
  - **SEC**: from N-PORT XML
  - **Tradefeeds**: from `mon1/2/3Flow.redemption` fields
- **Deduplication**: Yes - same priority system as sales
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - used as-is from either source

### 14. **reinvest**
- **Source**: SEC N-PORT-P filings (primary) + Tradefeeds API (fallback)
- **Collection Method**: Same hybrid approach as sales/redemptions
- **Processing**: 
  - **SEC**: from N-PORT XML
  - **Tradefeeds**: from `mon1/2/3Flow.reinvestment` fields
- **Deduplication**: Yes - same priority system
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - used as-is from either source

### 15. **net_flow**
- **Source**: Computed field
- **Collection Method**: N/A - computed from other fields
- **Processing**: Computed using `compute_net_flow()` function
- **Deduplication**: Inherits from source data
- **Data Overrides**: No overrides apply
- **Post Calculations**: `net_flow = sales + reinvest - redemptions`

### 16. **vol_of_flows**
- **Source**: Computed field
- **Collection Method**: N/A - computed from net_flow
- **Processing**: Computed using `compute_flow_volatility()` function
- **Deduplication**: Inherits from source data
- **Data Overrides**: No overrides apply
- **Post Calculations**: Rolling 12-month standard deviation of net_flow by class_id: `df.groupby("class_id")["net_flow"].transform(lambda x: x.rolling(window=12, min_periods=3).std())`

### 17-23. **MKT_RF, SMB, HML, RMW, CMA, RF, MOM**
- **Source**: Ken French Data Library (Dartmouth)
- **Collection Method**: 
  - **URLs**: 
    - FF5: `https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip`
    - Momentum: `https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_CSV.zip`
  - **Headers**: SEC headers (User-Agent required)
  - **Method**: HTTP downloads of ZIP files, extract CSV, parse with custom logic
- **Processing**: 
  - Downloads ZIP files, extracts CSV
  - Finds header lines containing "Mkt-RF" or "Mom"
  - Finds data start (lines beginning with 6-digit numbers)
  - Finds data end ("Annual Factors:" or EOF)
  - Parses date format from YYYYMM to month_end
  - Merges FF5 and Momentum data on month_end
- **Deduplication**: No deduplication needed - one record per month
- **Data Overrides**: No overrides apply
- **Post Calculations**: 
  - Values converted from percentage to decimal: `fac[c] = pd.to_numeric(fac[c], errors="coerce") / 100.0`
  - Date conversion: `ff5["month_end"] = ff5["date"] + pd.offsets.MonthEnd(0)`

### 24. **realized alpha**
- **Source**: Computed field
- **Collection Method**: N/A - computed from factor regressions
- **Processing**: Computed using `rolling_factor_regressions()` function
- **Deduplication**: Inherits from source data (critical deduplication prevents multicollinearity)
- **Data Overrides**: No overrides apply
- **Post Calculations**: 
  - 36-month rolling OLS regression with 30 minimum observations
  - `excess_return = return - RF`
  - `alpha_hat = sm.OLS(excess_return, sm.add_constant([MKT_RF, SMB, HML, RMW, CMA, MOM])).fit().params['const']`
  - Requires clean data (no NaN/infinite values)
  - Rolling window: examines window_df = g.iloc[max(0, i-36+1):i+1] for each observation

### 25. **adj_r_squared**
- **Source**: Computed field
- **Collection Method**: N/A - computed from factor regressions
- **Processing**: Same regression as realized alpha
- **Deduplication**: Inherits from source data
- **Data Overrides**: No overrides apply
- **Post Calculations**: R-squared from same 36-month rolling factor regression: `model.rsquared`

### 26. **net_expense_ratio_x**
- **Source**: SEC Risk/Return (RR) quarterly datasets
- **Collection Method**: 
  - **Location**: Local file system reads from `sec_rr_datasets/` directory
  - **Files**: Quarterly TSV files from SEC RR datasets
  - **Function**: `SECRRDataLoader.extract_expense_turnover()`
- **Processing**: 
  - Scans directory for quarterly TSV files
  - Loads and processes TSV data
  - Matches by series_id or class_id
  - Renamed from 'expense_ratio' to 'net_expense_ratio'
- **Deduplication**: Yes - `.drop_duplicates()` applied when merging
- **Data Overrides**: Yes - used as fallback if override file (`fees_turnover_override.csv`) doesn't contain data
- **Post Calculations**: None - used as-is from SEC RR datasets

### 27. **turnover_pct**
- **Source**: SEC Risk/Return (RR) quarterly datasets
- **Collection Method**: Same as net_expense_ratio_x
- **Processing**: 
  - Same extraction process as expense ratios
  - Renamed from 'turnover_rate' to 'turnover_pct'
- **Deduplication**: Yes - same as net_expense_ratio_x
- **Data Overrides**: Yes - same fallback logic as net_expense_ratio_x
- **Post Calculations**: None - used as-is from SEC RR datasets

### 28. **manager_tenure**
- **Source**: SEC N-1A filings (via manager_tenure.py module)
- **Collection Method**: 
  - **Function**: `get_manager_data_for_entities()` 
  - **Method**: Downloads N-1A filings using `fetch_latest_rr_doc_for_cik_filtered()`
  - **Years Attempted**: 2020, 2015, 2010, 2005, 1990 (in order until success)
- **Processing**: 
  - Downloads N-1A filing text
  - Regex pattern matching for manager tenure information:
    - `"(?:since|Since)\s+(\d{4})"`
    - `"has\s+(?:managed|been\s+(?:managing|the\s+portfolio\s+manager))[^.]{0,100}since\s+(?:\w+\s+)?(\d{4})"`
    - `"Portfolio\s+Manager[^.]{0,50}since\s+(?:\w+\s+)?(\d{4})"`
    - `"joined[^.]{0,100}(?:in|since)\s+(?:\w+\s+)?(\d{4})"`
    - `"appointed[^.]{0,100}(?:in|since)\s+(?:\w+\s+)?(\d{4})"`
  - Uses most recent start year found
- **Deduplication**: Yes - duplicates handled during merge on series_id
- **Data Overrides**: Yes - can be overridden via `fees_turnover_override.csv` file
- **Post Calculations**: `tenure_years = filing_date.year - start_year` (minimum 0.0 years)

### 29. **fund_age**
- **Source**: SEC N-1A filings (via manager_tenure.py module)
- **Collection Method**: Same as manager_tenure
- **Processing**: 
  - Same N-1A filing download process
  - Regex pattern matching for inception dates:
    - `"commenced\s+operations[^.]{0,50}(?:on|in)\s+(\w+\s+\d{1,2},?\s+\d{4})"`
    - `"inception[^:]{0,20}:?\s*(\w+\s+\d{1,2},?\s+\d{4})"`
    - `"established[^.]{0,50}(?:on|in)?\s+(\w+\s+\d{1,2},?\s+\d{4})"`
    - `"(?:fund|portfolio)\s+began\s+operations[^.]{0,50}(\w+\s+\d{1,2},?\s+\d{4})"`
  - Parses dates using multiple formats: "%B %d %Y", "%b %d %Y", "%B %Y", "%b %Y"
- **Deduplication**: Same as manager_tenure
- **Data Overrides**: Yes - same as manager_tenure
- **Post Calculations**: `fund_age = (filing_date - inception_date).days / 365.25`

### 30. **inception_date**
- **Source**: SEC N-1A filings (via manager_tenure.py module)
- **Collection Method**: Same as manager_tenure/fund_age
- **Processing**: Same regex extraction as fund_age, returns date as "YYYY-MM-DD" string
- **Deduplication**: Same as manager_tenure
- **Data Overrides**: Yes - same as manager_tenure
- **Post Calculations**: None - used as-is after parsing

### 31. **asof**
- **Source**: SEC filing metadata or computed field
- **Collection Method**: From filing metadata in manager_tenure module
- **Processing**: Filing date from N-1A document metadata (`hit.get("filing_date")`)
- **Deduplication**: Inherits from source data
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - metadata field indicating data freshness

### 32. **class_id**
- **Source**: SEC series/class mapping
- **Collection Method**: Same CSV downloads as series_id (field #3)
- **Processing**: 
  - Primary identifier extracted from SeriesClassMapper
  - Used as key for all merge operations
  - Column consolidation logic handles merge duplicates (class_id_x/class_id_y)
- **Deduplication**: Used as key field for deduplication
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - primary key field

### 33. **cik**
- **Source**: Pilot configuration YAML + SEC filing metadata
- **Collection Method**: File read from config + SEC filings
- **Processing**: 
  - CIK extracted from pilot config and SEC filing metadata
  - Column consolidation logic: `facts_with_factors['cik'] = facts_with_factors['cik_x'].fillna(facts_with_factors['cik_y'])`
- **Deduplication**: Consolidated when duplicate cik_x/cik_y columns exist from merges
- **Data Overrides**: No overrides apply
- **Post Calculations**: None - identifier field

### 34. **net_expense_ratio_y**
- **Source**: Data override file (`fees_turnover_override.csv`)
- **Collection Method**: 
  - **Method**: CSV file read using `apply_data_overrides()`
  - **Location**: Local filesystem path specified by `--fees` argument
- **Processing**: 
  - Applied as manual override after SEC RR data integration
  - Uses DataOverrideProcessor to apply class_id/series_id level overrides
- **Deduplication**: Yes - handled during override application
- **Data Overrides**: Yes - this IS the override data source (primary, not fallback)
- **Post Calculations**: None - used as-is from override file

### 35. **tna_proxy**
- **Source**: Computed field (derived from total_investments)
- **Collection Method**: N/A - computed from total_investments
- **Processing**: 
  - Renamed copy of total_investments: `tna_proxy = facts_with_factors[['class_id', 'month_end', 'total_investments']].rename(columns={'total_investments': 'tna'})`
  - Used specifically for value_added calculation
- **Deduplication**: Inherits from total_investments
- **Data Overrides**: No overrides apply
- **Post Calculations**: Direct copy/rename of total_investments field

### 36. **tna_lag**
- **Source**: Computed field (derived from tna_proxy)
- **Collection Method**: N/A - computed from tna_proxy
- **Processing**: Part of value_added calculation in `value_added()` function
- **Deduplication**: Inherits from source data
- **Data Overrides**: No overrides apply
- **Post Calculations**: Lagged tna_proxy by 1 period: `merged["tna_lag"] = merged.groupby("class_id")["tna_proxy"].shift(1)`

### 37. **value_added**
- **Source**: Computed field
- **Collection Method**: N/A - computed from multiple fields
- **Processing**: Computed using `value_added()` function
- **Deduplication**: Inherits from source data
- **Data Overrides**: No overrides apply
- **Post Calculations**: 
  - `value_added = (alpha_hat - net_expense_ratio) * tna_lag` when all components are available
  - Requires non-null values for realized alpha, net_expense_ratio, and tna_lag
  - Returns None if any required component is missing

---

## Detailed Technical Components

### Tradefeeds API Specifications

#### Base Configuration
- **Base URL**: `https://data.tradefeeds.com/api/v1`
- **Authentication**: API key passed as URL parameter (`key=api_key`)
- **Rate Limiting**: 2-second intervals between requests (1 request per 2 seconds)
- **Session Management**: Uses `requests.Session()` for connection pooling

#### Primary Endpoints Used

1. **Historical OHLCV Endpoint**
   - **URL**: `{BASE_URL}/historical_ohlcv`
   - **Method**: HTTP GET
   - **Headers**: `{'accept': 'application/json'}`
   - **Parameters**: 
     - `key`: API key (required)
     - `ticker`: Mutual fund ticker symbol (required)
     - `date_from`: Start date in YYYY-MM-DD format (optional)
     - `date_to`: End date in YYYY-MM-DD format (optional)
   - **Response Structure**: 
     ```json
     {
       "result": {
         "output": {
           "daily_stock_data": [
             {
               "date": "YYYY-MM-DD",
               "open": float,
               "high": float,
               "low": float,
               "close": float,
               "adjustclose": float,
               "volume": int
             }
           ]
         }
       }
     }
     ```

2. **Mutual Fund Periodical Returns Endpoint** (for flow data fallback)
   - **URL**: `{BASE_URL}/mutual_fund_periodical_returns`
   - **Method**: HTTP GET
   - **Headers**: `{'accept': 'application/json'}`
   - **Parameters**:
     - `key`: API key (required)
     - `seriesId`: SEC series ID (required)
     - `date_from`: Start date (optional)
     - `date_to`: End date (optional)
   - **Response Structure**:
     ```json
     {
       "result": {
         "output": [
           {
             "attributes": {
               "seriesId": "string",
               "repPdDate": "YYYY-MM-DD"
             },
             "return_info": {
               "mon1Flow": {
                 "sales": float,
                 "redemption": float,
                 "reinvestment": float
               },
               "mon2Flow": {...},
               "mon3Flow": {...}
             }
           }
         ]
       }
     }
     ```

3. **API Connectivity Test Endpoint**
   - **URL**: `{BASE_URL}/mutual_fund_information`
   - **Parameters**: `{'key': api_key, 'regCik': '1137095'}`
   - **Purpose**: Validates API connectivity and authentication

### SeriesClassMapper Technical Details

#### Data Sources Priority
1. **Primary Sources** (direct CSV downloads):
   - `https://www.sec.gov/files/investment/data/other/investment-company-series-class-information/investment-company-series-class-{YEAR}.csv`
   - Attempts years: 2025, 2024, 2023, 2022, 2021
   - Variations: `investment-company-series-class-{year}.csv` and `investment_company_series_class_{year}.csv`

2. **Fallback Discovery**:
   - Scrapes `https://www.sec.gov/about/opendatasetsshtmlinvestment_company`
   - XPath: `"//a[@href]"` looking for links containing "series" and "class" ending in ".csv"

#### Column Standardization Process
- **CIK Columns**: `["cik_number", "cik", "registrant_cik", "central_index_key", "company_cik"]`
- **Series ID Columns**: `["series_id", "seriesid", "series_class_id", "dei_series_id", "series_identifier"]`
- **Class ID Columns**: `["class_id", "class_contract_id", "classcontractid", "dei_class_contract_id", "class_identifier"]`
- **Ticker Columns**: `["class_ticker", "ticker", "class_contract_ticker_symbol", "symbol"]`

#### Processing Logic
```python
# Column normalization
combined_df.columns = [col.lower().replace(" ", "_").replace("-", "_") for col in combined_df.columns]

# CIK processing
standard_mapping["cik"] = combined_df[cik_col].astype(str).str.strip().str.lstrip("0")

# Series/Class ID processing  
standard_mapping["series_id"] = combined_df[series_col].astype(str).str.strip().str.upper()
standard_mapping["class_id"] = combined_df[class_col].astype(str).str.strip().str.upper()
```

#### Lookup Structure
```python
# Ticker lookup structure created by _build_ticker_lookup()
ticker_lookup = {
    "S000002594": [
        {"class_id": "C000007113", "ticker": "VHCOX"},
        {"class_id": "C000007114", "ticker": "VHCAX"}
    ],
    "S000002568": [
        {"class_id": "C000007070", "ticker": "VPMCX"},
        {"class_id": "C000007071", "ticker": "VPMAX"}
    ]
}
```

### Manager Tenure Extraction Details

#### N-1A Filing Access Strategy
- **Function**: `get_manager_data_for_entities()`
- **Filing Type**: N-1A (annual mutual fund reports)
- **Year Strategy**: Attempts multiple years in order: 2020, 2015, 2010, 2005, 1990
- **Fallback Logic**: If one year fails, tries next year until successful document download

#### Regex Patterns for Manager Tenure
```python
patterns = [
    r"(?:since|Since)\s+(\d{4})",  # "since YYYY" or "Since YYYY"
    r"has\s+(?:managed|been\s+(?:managing|the\s+portfolio\s+manager))[^.]{0,100}since\s+(?:\w+\s+)?(\d{4})",  # Complex manager descriptions
    r"Portfolio\s+Manager[^.]{0,50}since\s+(?:\w+\s+)?(\d{4})",  # "Portfolio Manager since YYYY"
    r"joined[^.]{0,100}(?:in|since)\s+(?:\w+\s+)?(\d{4})",  # "joined... in YYYY"
    r"appointed[^.]{0,100}(?:in|since)\s+(?:\w+\s+)?(\d{4})",  # "appointed... in YYYY"
]
```

#### Fund Inception Date Patterns
```python
patterns = [
    r"commenced\s+operations[^.]{0,50}(?:on|in)\s+(\w+\s+\d{1,2},?\s+\d{4})",  # "commenced operations on [Date]"
    r"inception[^:]{0,20}:?\s*(\w+\s+\d{1,2},?\s+\d{4})",  # "inception date: [Date]"
    r"established[^.]{0,50}(?:on|in)?\s+(\w+\s+\d{1,2},?\s+\d{4})",  # "established [Date]"
    r"(?:fund|portfolio)\s+began\s+operations[^.]{0,50}(\w+\s+\d{1,2},?\s+\d{4})",  # "fund began operations [Date]"
]
```

#### Date Format Parsing
```python
# Supported date formats for inception dates
formats = ["%B %d %Y", "%b %d %Y", "%B %Y", "%b %Y"]
# Examples: "January 15 2020", "Jan 15 2020", "January 2020", "Jan 2020"
```

### Comprehensive Deduplication Strategy

#### 1. **Tradefeeds Returns Data Deduplication**
```python
# In fetch_with_chunking() method
sort_cols = ['class_id', 'month_end']
if 'filing_date' in combined_df.columns:
    sort_cols.append('filing_date')
elif 'report_date' in combined_df.columns:
    sort_cols.append('report_date')

combined_df = combined_df.sort_values(sort_cols)
combined_df = combined_df.drop_duplicates(subset=['class_id', 'month_end'], keep='last')
```

#### 2. **Flow Data Source Priority Deduplication**
```python
# SEC data preferred over Tradefeeds
combined_df['source_priority'] = combined_df['data_source'].map({'sec': 0, 'tradefeeds': 1}).fillna(1)
combined_df = combined_df.sort_values(['class_id', 'month_end', 'source_priority'])
combined_df = combined_df.drop_duplicates(subset=['class_id', 'month_end'], keep='first')  # Keep SEC (priority 0)
```

#### 3. **SEC N-PORT TNA Data Deduplication**
```python
# Keep most recent filing for each class_id-month_end
tna_data = tna_data.sort_values(['class_id', 'month_end', 'filing_date'])
tna_data = tna_data.drop_duplicates(subset=['class_id', 'month_end'], keep='last')
```

#### 4. **Factor Data Integration Deduplication** (Critical Fix)
```python
# Critical fix to prevent multicollinearity in factor regressions
factor_cols = ["MKT_RF", "SMB", "HML", "RMW", "CMA", "RF", "MOM"]
facts_with_factors['factor_completeness'] = facts_with_factors[factor_cols].count(axis=1)
facts_with_factors = facts_with_factors.sort_values(['class_id', 'month_end', 'factor_completeness'])
facts_with_factors = facts_with_factors.drop_duplicates(subset=['class_id', 'month_end'], keep='last')
facts_with_factors = facts_with_factors.drop(columns=['factor_completeness'])
```

#### 5. **Column Consolidation After Merges**
```python
# Handle duplicate columns from multiple merges
if 'class_id_x' in facts_with_factors.columns and 'class_id_y' in facts_with_factors.columns:
    facts_with_factors['class_id'] = facts_with_factors['class_id_x']
    facts_with_factors = facts_with_factors.drop(columns=['class_id_x', 'class_id_y'])

# Similar logic for CIK columns
if 'cik_x' in facts_with_factors.columns and 'cik_y' in facts_with_factors.columns:
    facts_with_factors['cik'] = facts_with_factors['cik_x'].fillna(facts_with_factors['cik_y'])
    facts_with_factors = facts_with_factors.drop(columns=['cik_x', 'cik_y'])
```

#### 6. **SEC RR Data Deduplication**
```python
# When merging expense ratio and turnover data
er_turnover_df[['series_id', 'net_expense_ratio', 'turnover_pct']].drop_duplicates()
# or
er_turnover_df[['class_id', 'net_expense_ratio', 'turnover_pct']].drop_duplicates()
```

### Factor Regression Implementation

#### Rolling Window Logic
```python
def rolling_factor_regressions(df: DataFrame, window: int = 36, min_obs: int = 30) -> DataFrame:
    results = []
    grouped = df.groupby("class_id")
    
    for cid, g in grouped:
        g = g.sort_values("month_end").reset_index(drop=True)
        for i in range(len(g)):
            # Rolling window: current observation back to window-1 months
            window_df = g.iloc[max(0, i-window+1):i+1]
            
            if len(window_df) < min_obs:
                continue  # Skip if insufficient observations
            
            # Clean data: remove NaN/infinite values
            required_cols = ["return", "RF", "MKT_RF", "SMB", "HML", "RMW", "CMA", "MOM"]
            clean_df = window_df[required_cols].dropna()
            
            if len(clean_df) < min_obs or not np.isfinite(clean_df.values).all():
                continue
                
            # Factor regression: excess_return = alpha + beta * factors
            y = clean_df["return"] - clean_df["RF"]  # Excess return
            X = clean_df[["MKT_RF","SMB","HML","RMW","CMA","MOM"]]
            X = sm.add_constant(X)  # Add intercept
            
            try:
                model = sm.OLS(y, X).fit()
                # Extract results
                results.append({
                    "class_id": cid,
                    "month_end": g.loc[i,"month_end"],
                    "alpha_hat": model.params["const"],  # Intercept = alpha
                    "R2": model.rsquared
                })
            except Exception:
                continue  # Skip failed regressions
```

### Data Override System

#### Override File Structure
- **File**: `fees_turnover_override.csv` (or specified by `--fees` argument)
- **Columns**: `ticker,class_id,net_expense_ratio,turnover_pct,asof`
- **Processing**: Applied via `apply_data_overrides()` function
- **Logic**: Manual corrections applied AFTER SEC RR data integration
- **Scope**: Primarily for expense ratios and turnover rates that couldn't be obtained from SEC sources

#### Override Application Logic
```python
def apply_data_overrides(df: DataFrame, override_file: str) -> DataFrame:
    if Path(override_file).exists():
        override_df = pd.read_csv(override_file)
        # Apply overrides based on available identifiers (class_id, ticker, series_id)
        # Replaces existing values with manual corrections
    return df
```

---

## Data Quality and Coverage Implications

### Expected Coverage Patterns
1. **Returns Data**: Nearly 100% for configured tickers (limited by Tradefeeds API availability)
2. **Flow Data**: Variable (~10-40% typical) due to hybrid SEC/Tradefeeds approach
3. **TNA Data**: Moderate (~20-50%) based on SEC N-PORT filing availability
4. **Factor Data**: 100% for available months (Ken French data completeness)
5. **Realized Alpha**: Limited by 36-month rolling requirement + data completeness
6. **Expense Ratios**: High (~90%+) due to SEC RR data + manual overrides
7. **Manager Tenure**: Moderate (~30-60%) based on N-1A filing accessibility and regex matching success

### Critical Success Factors
1. **Deduplication**: Prevents multicollinearity in factor regressions
2. **Rate Limiting**: Ensures API quota compliance and data completeness
3. **Hybrid Approach**: Maximizes data coverage through multiple sources
4. **Error Handling**: Graceful fallbacks when primary sources fail
5. **Data Validation**: Quality checks at each processing stage

---

## File Generated: `COMPREHENSIVE_DATA_EXTRACTION_ANALYSIS.md`
This document provides complete technical specifications for the `pilot_pipeline_tradefeeds.py` data extraction process.